{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spotlight\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk import Tree\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import ne_chunk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp_main = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_coref_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_pars(text):\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "              [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_NER(text):\n",
    "    save_ner = []\n",
    "    #parse entire text to find sentence\n",
    "    pars = nlp_main(unicode(text))\n",
    "    #for each sentence find named entities\n",
    "    for i in pars.sents:\n",
    "        a = nlp_main(unicode(i))\n",
    "        #parse sentence to find the text and NER labels\n",
    "        for j in nlp_main(unicode(i)).ents:\n",
    "            #save all values except for junk text '\\n'\n",
    "            if j.text != '\\n':\n",
    "                save_ner.extend([str(j.text) +' ,'+str(j.label_)])\n",
    "    return save_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def POS_tags(text):\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_amod(text):\n",
    "    token_head_text = []\n",
    "    token_head_text_pos = []\n",
    "    token_text = []\n",
    "    token_pos = []\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'amod':\n",
    "            token_head_text.extend([token.head.text])\n",
    "            token_text.extend([token.text])\n",
    "            token_pos.extend([token.pos_])\n",
    "            token_head_text_pos.extend([token.head.pos_])\n",
    "    return token_head_text,token_text,token_pos,token_head_text_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_advmod(text):\n",
    "    token_head_text = []\n",
    "    token_text = []\n",
    "    token_pos = []\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'advmod':\n",
    "            token_head_text.extend([token.text])\n",
    "            token_text.extend([token.head.text])\n",
    "            token_pos.extend([token.head.pos_])\n",
    "    return token_head_text,token_text,token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_compound(text):\n",
    "    token_head_text = []\n",
    "    token_text = []\n",
    "    token_pos = []\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'compound':\n",
    "            token_head_text.extend([token.text])\n",
    "            token_text.extend([token.head.text])\n",
    "            token_pos.extend([token.head.pos_])\n",
    "    return token_head_text,token_text,token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Third_rule(text):\n",
    "    token_text_amod,token_head_text_amod,token_head_pos_amod,ignore = dep_amod(text)\n",
    "    token_text_advmod,token_head_text_advmod,token_head_pos_advmod = dep_advmod(text)\n",
    "    token_head_text_amod_new = []\n",
    "    final_result = []\n",
    "    counter_dep_advmod=0\n",
    "    for i,j,k in zip(token_text_amod,token_head_text_amod,token_head_pos_amod):\n",
    "        if k == 'ADJ' and token_head_text_advmod[counter_dep_advmod] == j:\n",
    "            token_head_text_amod_new.extend([token_text_advmod[counter_dep_advmod]+ ' ' +token_head_text_advmod[counter_dep_advmod]])\n",
    "            counter_dep_advmod+=1\n",
    "        else:\n",
    "            token_head_text_amod_new.extend([j])\n",
    "    token_head_text_amod = token_head_text_amod_new\n",
    "    del token_head_text_amod_new\n",
    "    for i,j in zip(token_text_amod,token_head_text_amod):\n",
    "        final_result.extend([str(i)+': '+str(j)])\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0cf682a25b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mThird_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "Third_rule(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fourth_rule(text):\n",
    "    token_text_amod,token_head_text_amod,ignore,token_head_text_pos = dep_amod(text)\n",
    "    token_text_compound,token_head_text_compound,token_head_pos_compound = dep_compound(text)\n",
    "    token_head_text_amod_new = []\n",
    "    final_result = []\n",
    "    counter_dep_compound=0\n",
    "    for i,j,k in zip(token_text_amod,token_head_text_amod,token_head_text_pos):\n",
    "        if k == 'VERB' and token_head_text_compound[counter_dep_compound] == i:\n",
    "            token_head_text_amod_new.extend([token_text_compound[counter_dep_compound]+ ' ' +token_head_text_compound[counter_dep_compound]])\n",
    "            counter_dep_compound+=1\n",
    "        else:\n",
    "            token_head_text_amod_new.extend([i])\n",
    "    token_text_amod = token_head_text_amod_new\n",
    "    del token_head_text_amod_new\n",
    "    for i,j in zip(token_text_amod,token_head_text_amod):\n",
    "        final_result.extend([str(i)+': '+str(j)])\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Staff was awesome, Doctor incredible. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_acomp(text):\n",
    "    token_text = []\n",
    "    token_head_text = []\n",
    "    token_head_pos = []\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'acomp':\n",
    "            token_text.extend([token.text])\n",
    "            token_head_text.extend([token.head.text])\n",
    "            token_head_pos.extend([token.head.pos_])\n",
    "    return token_text,token_head_text,token_head_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dep_nsubj(text):\n",
    "    token_text = []\n",
    "    token_head_text = []\n",
    "    token_head_pos = []\n",
    "    doc = nlp_main(unicode(text))\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            token_text.extend([token.text])\n",
    "            token_head_text.extend([token.head.text])\n",
    "            token_head_pos.extend([token.head.pos_])\n",
    "    return token_text,token_head_text,token_head_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compiled_second(text):\n",
    "    token_text_nsub,token_head_text_nsub,token_head_pos_nsub = dep_nsubj(text)\n",
    "    token_text_acomp,token_head_text_acomp,token_head_pos_acomp = dep_acomp(text)\n",
    "    token_head_text_nsub_new = []\n",
    "    final_result = []\n",
    "    counter_dep_acomp=0\n",
    "    for i,j,k in zip(token_text_nsub,token_head_text_nsub,token_head_pos_nsub):\n",
    "        if k == 'VERB' and token_head_text_nsub[counter_dep_acomp] == j:\n",
    "            token_head_text_nsub_new.extend([token_text_acomp[counter_dep_acomp]])\n",
    "            counter_dep_acomp+=1\n",
    "        else:\n",
    "            token_head_text_nsub_new.extend([j])\n",
    "    token_head_text_nsub = token_head_text_nsub_new\n",
    "    del token_head_text_nsub_new\n",
    "    for i,j in zip(token_text_nsub,token_head_text_nsub):\n",
    "        final_result.extend([str(i)+': '+str(j)])\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compiled_second(text='Staff was awesome, Doctor incredible. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_v1(text):\n",
    "    grammar = \"\"\"P2:{<NN><.*>?<RB><JJ>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "    #tokenize to sentence and word. Find part of speech tag\n",
    "    text_sent_tok = nltk.sent_tokenize(text)\n",
    "    text_word_tok = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in text_sent_tok]\n",
    "    for sent_obj in text_word_tok:\n",
    "        pars_tree=cp.parse(sent_obj)\n",
    "        counter_first = 0\n",
    "        sent_p1 = ''\n",
    "        sent_p2 = ''\n",
    "        sent_full = ''\n",
    "        ##parse tree to find individual grammar subtrees\n",
    "        for subtree in pars_tree.subtrees():\n",
    "            if (subtree.label()==\"P2\"):\n",
    "                #print(subtree.pos)\n",
    "                for eachleaf in subtree.leaves():\n",
    "                    if eachleaf[1] != 'VBZ':\n",
    "                        sent_p2 +=eachleaf[0]+' '\n",
    "                    else:\n",
    "                        sent_p2 +=': '\n",
    "                sent_full = sent_p1 + sent_p2\n",
    "    #    if len(sent_full)>0:\n",
    "    #        print(sent_full.rstrip(\" \"))\n",
    "        if len(sent_full)>0:\n",
    "            return sent_full.rstrip(\" \").replace(\" :\",\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_v2(text):\n",
    "    grammar = \"\"\"P2:{<NN><NN><.*>?<JJ>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "    #tokenize to sentence and word. Find part of speech tag\n",
    "    text_sent_tok = nltk.sent_tokenize(text)\n",
    "    text_word_tok = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in text_sent_tok]\n",
    "    for sent_obj in text_word_tok:\n",
    "        pars_tree=cp.parse(sent_obj)\n",
    "        counter_first = 0\n",
    "        sent_p1 = ''\n",
    "        sent_p2 = ''\n",
    "        sent_full = ''\n",
    "        ##parse tree to find individual grammar subtrees\n",
    "        for subtree in pars_tree.subtrees():\n",
    "            if (subtree.label()==\"P2\"):\n",
    "                #print(subtree.pos)\n",
    "                for eachleaf in subtree.leaves():\n",
    "                    if eachleaf[1] != 'VBD':\n",
    "                        sent_p2 +=eachleaf[0]+' '\n",
    "                    else:\n",
    "                        sent_p2 +=': '\n",
    "                sent_full = sent_p1 + sent_p2\n",
    "    #    if len(sent_full)>0:\n",
    "    #        print(sent_full.rstrip(\" \"))\n",
    "        if len(sent_full)>0:\n",
    "            return sent_full.rstrip(\" \").replace(\" :\",\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_v3(text):\n",
    "    grammar = \"\"\"P2:{<NN><NN><VBD><JJ><CC><NN>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "    #tokenize to sentence and word. Find part of speech tag\n",
    "    text_sent_tok = nltk.sent_tokenize(text)\n",
    "    text_word_tok = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in text_sent_tok]\n",
    "    for sent_obj in text_word_tok:\n",
    "        #print(sent_obj)\n",
    "        pars_tree=cp.parse(sent_obj)\n",
    "        counter_first = 0\n",
    "        sent_p1 = ''\n",
    "        sent_p2 = ''\n",
    "        sent_full = ''\n",
    "        ##parse tree to find individual grammar subtrees\n",
    "        for subtree in pars_tree.subtrees():\n",
    "            if (subtree.label()==\"P2\"):\n",
    "                #print(subtree.pos)\n",
    "                for eachleaf in subtree.leaves():\n",
    "                    if eachleaf[1] != 'VBD' and eachleaf[1] != 'JJ' and eachleaf[1] != 'CC':\n",
    "                        sent_p2 +=eachleaf[0]+' '\n",
    "                    if eachleaf[1] == 'CC':\n",
    "                        sent_p2 +=': '\n",
    "                sent_full = sent_p1 + sent_p2\n",
    "        if len(sent_full)>0:\n",
    "            return sent_full.rstrip(\" \").replace(\" :\",\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MasterFirst(text):\n",
    "    try:\n",
    "        final_vals = ''\n",
    "        if len(first_v1(text))>0:\n",
    "            final_vals = first_v1(text)\n",
    "        if len(first_v2(text))>0 and len(final_vals)>0:\n",
    "            final_vals = final_vals + '; ' + first_v2(text)\n",
    "        elif len(first_v2(text))>0:\n",
    "            final_vals = first_v2(text)\n",
    "        if len(first_v3(text))>0 and len(final_vals)>0:\n",
    "            final_vals = final_vals + '; ' + first_v3(text)\n",
    "        elif len(first_v3(text))>0:\n",
    "            final_vals = first_v3(text)\n",
    "    except:\n",
    "        pass\n",
    "    return final_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FinalPiece(text):\n",
    "    if len(MasterFirst(text))>0:\n",
    "        return MasterFirst(text)\n",
    "    elif len(MasterFirst(text))==0:\n",
    "        try:\n",
    "            return compiled_second(text)\n",
    "        except:\n",
    "            pass\n",
    "    elif (len(MasterFirst(text))==0) and (len(compiled_second(text))==0):\n",
    "        try:\n",
    "            return Third_rule(text)\n",
    "        except:\n",
    "            return Fourth_rule(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = 'The office is very clean and customer service was great and speedy'\n",
    "#text = 'Staff was awesome, Doctor incredible. '\n",
    "#text = 'very clear instructions'\n",
    "text = 'Beautiful office surroundings & wonderful receptionists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FinalPiece(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv('C:\\Users\\Dell\\Downloads\\Feature-OpinionExtraction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_output = []\n",
    "my_val = ''\n",
    "for i in range(dat.shape[0]):\n",
    "    #print(dat.Text.iloc[i])\n",
    "    my_val += str(\"Row id: \") + str(i) + ': ' +str(FinalPiece(dat.Text.iloc[i]))\n",
    "    final_output.extend([my_val])\n",
    "    my_val = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Row id: 0: None',\n",
       " 'Row id: 1: None',\n",
       " 'Row id: 2: None',\n",
       " 'Row id: 3: None',\n",
       " 'Row id: 4: None',\n",
       " 'Row id: 5: None',\n",
       " 'Row id: 6: staff were so gentle',\n",
       " 'Row id: 7: None',\n",
       " \"Row id: 8: ['doctor: professional']\",\n",
       " 'Row id: 9: None',\n",
       " 'Row id: 10: doctor was very professional',\n",
       " 'Row id: 11: doctor was very professional',\n",
       " \"Row id: 12: ['doctor: Professional']\",\n",
       " 'Row id: 13: None',\n",
       " 'Row id: 14: []',\n",
       " \"Row id: 15: ['Allie: friendly']\",\n",
       " 'Row id: 16: None',\n",
       " \"Row id: 17: ['staff: courteous']\",\n",
       " \"Row id: 18: ['staff: courteous']\",\n",
       " 'Row id: 19: None',\n",
       " 'Row id: 20: []',\n",
       " 'Row id: 21: None',\n",
       " 'Row id: 22: []',\n",
       " 'Row id: 23: None',\n",
       " 'Row id: 24: []',\n",
       " 'Row id: 25: None',\n",
       " 'Row id: 26: None',\n",
       " 'Row id: 27: None',\n",
       " 'Row id: 28: None',\n",
       " \"Row id: 29: ['Ashley: nice']\",\n",
       " \"Row id: 30: ['we: confident']\",\n",
       " 'Row id: 31: None',\n",
       " 'Row id: 32: None',\n",
       " 'Row id: 33: None',\n",
       " 'Row id: 34: None',\n",
       " 'Row id: 35: None',\n",
       " 'Row id: 36: []',\n",
       " 'Row id: 37: None',\n",
       " \"Row id: 38: ['Christina: pleasant']\",\n",
       " 'Row id: 39: None',\n",
       " 'Row id: 40: None',\n",
       " 'Row id: 41: []',\n",
       " \"Row id: 42: ['Connie: listener', 'i: intuitive']\",\n",
       " \"Row id: 43: ['Corbin: friendly']\",\n",
       " 'Row id: 44: None',\n",
       " \"Row id: 45: ['Saleh: helpful', 'both: calm']\",\n",
       " 'Row id: 46: helpful , very thorough',\n",
       " \"Row id: 47: ['Duchene: friendly']\",\n",
       " \"Row id: 48: ['Dawna: attentive']\",\n",
       " \"Row id: 49: ['Dee: pleasant']\",\n",
       " \"Row id: 50: ['Dee: professional']\",\n",
       " \"Row id: 51: ['Doctor: better']\",\n",
       " 'Row id: 52: doctor was very thorough',\n",
       " 'Row id: 53: None',\n",
       " 'Row id: 54: None',\n",
       " \"Row id: 55: ['Cheng: caring']\",\n",
       " 'Row id: 56: None',\n",
       " 'Row id: 57: None',\n",
       " \"Row id: 58: ['G: great']\",\n",
       " 'Row id: 59: caring and very quick',\n",
       " \"Row id: 60: ['Eppes: upbeat']\",\n",
       " 'Row id: 61: None',\n",
       " 'Row id: 62: None',\n",
       " 'Row id: 63: None',\n",
       " 'Row id: 64: None',\n",
       " 'Row id: 65: None',\n",
       " \"Row id: 66: ['Gomez: kind']\",\n",
       " \"Row id: 67: ['G: easy']\",\n",
       " 'Row id: 68: None',\n",
       " 'Row id: 69: None',\n",
       " 'Row id: 70: None',\n",
       " 'Row id: 71: None',\n",
       " 'Row id: 72: None',\n",
       " 'Row id: 73: None',\n",
       " 'Row id: 74: None',\n",
       " 'Row id: 75: None',\n",
       " 'Row id: 76: None',\n",
       " \"Row id: 77: ['Meonch: helpful']\",\n",
       " 'Row id: 78: None',\n",
       " \"Row id: 79: ['Neil: knowledgeable']\",\n",
       " 'Row id: 80: None',\n",
       " 'Row id: 81: None',\n",
       " \"Row id: 82: ['Sugar: knowledgeable']\",\n",
       " 'Row id: 83: []',\n",
       " 'Row id: 84: None',\n",
       " 'Row id: 85: None',\n",
       " 'Row id: 86: None',\n",
       " \"Row id: 87: ['Erin: nice']\",\n",
       " 'Row id: 88: None',\n",
       " 'Row id: 89: None',\n",
       " 'Row id: 90: []',\n",
       " 'Row id: 91: None',\n",
       " 'Row id: 92: []',\n",
       " 'Row id: 93: doctor before very helpful',\n",
       " 'Row id: 94: None',\n",
       " 'Row id: 95: None',\n",
       " \"Row id: 96: ['I: pleased']\",\n",
       " 'Row id: 97: None',\n",
       " 'Row id: 98: None',\n",
       " 'Row id: 99: None',\n",
       " 'Row id: 100: None',\n",
       " 'Row id: 101: None',\n",
       " 'Row id: 102: None',\n",
       " 'Row id: 103: None',\n",
       " 'Row id: 104: interaction , just distant',\n",
       " 'Row id: 105: None',\n",
       " 'Row id: 106: None',\n",
       " 'Row id: 107: None',\n",
       " 'Row id: 108: []',\n",
       " 'Row id: 109: None',\n",
       " 'Row id: 110: None',\n",
       " 'Row id: 111: None',\n",
       " 'Row id: 112: None',\n",
       " 'Row id: 113: None',\n",
       " 'Row id: 114: None',\n",
       " 'Row id: 115: None',\n",
       " 'Row id: 116: None',\n",
       " 'Row id: 117: None',\n",
       " 'Row id: 118: None',\n",
       " 'Row id: 119: None',\n",
       " \"Row id: 120: ['I: delighted']\",\n",
       " \"Row id: 121: ['I: impressed']\",\n",
       " 'Row id: 122: None',\n",
       " 'Row id: 123: None',\n",
       " 'Row id: 124: None',\n",
       " 'Row id: 125: None',\n",
       " 'Row id: 126: None',\n",
       " 'Row id: 127: None',\n",
       " 'Row id: 128: None',\n",
       " 'Row id: 129: None',\n",
       " 'Row id: 130: None',\n",
       " 'Row id: 131: None',\n",
       " 'Row id: 132: None',\n",
       " 'Row id: 133: None',\n",
       " 'Row id: 134: None',\n",
       " \"Row id: 135: ['Laura: unprofessional']\",\n",
       " 'Row id: 136: None',\n",
       " 'Row id: 137: []',\n",
       " 'Row id: 138: None',\n",
       " 'Row id: 139: None',\n",
       " 'Row id: 140: []',\n",
       " 'Row id: 141: None',\n",
       " 'Row id: 142: []',\n",
       " 'Row id: 143: []',\n",
       " 'Row id: 144: None',\n",
       " 'Row id: 145: None',\n",
       " 'Row id: 146: None',\n",
       " 'Row id: 147: doctor was very good',\n",
       " 'Row id: 148: None',\n",
       " 'Row id: 149: None',\n",
       " 'Row id: 150: None',\n",
       " 'Row id: 151: Nurse was Very Good',\n",
       " 'Row id: 152: appointment so spent',\n",
       " 'Row id: 153: None',\n",
       " 'Row id: 154: None',\n",
       " 'Row id: 155: None',\n",
       " 'Row id: 156: None',\n",
       " 'Row id: 157: []',\n",
       " 'Row id: 158: None',\n",
       " \"Row id: 159: ['Murray: excellent']\",\n",
       " 'Row id: 160: None',\n",
       " 'Row id: 161: None',\n",
       " 'Row id: 162: []',\n",
       " 'Row id: 163: None',\n",
       " \"Row id: 164: ['Staff: concerned']\",\n",
       " 'Row id: 165: system was really cool',\n",
       " 'Row id: 166: dentist was so pleasant',\n",
       " 'Row id: 167: None',\n",
       " 'Row id: 168: None',\n",
       " 'Row id: 169: None',\n",
       " 'Row id: 170: None',\n",
       " 'Row id: 171: None',\n",
       " 'Row id: 172: None',\n",
       " 'Row id: 173: None',\n",
       " 'Row id: 174: None',\n",
       " 'Row id: 175: None',\n",
       " 'Row id: 176: None',\n",
       " 'Row id: 177: None',\n",
       " 'Row id: 178: None',\n",
       " 'Row id: 179: None',\n",
       " 'Row id: 180: None',\n",
       " 'Row id: 181: None',\n",
       " 'Row id: 182: None',\n",
       " 'Row id: 183: None',\n",
       " 'Row id: 184: None',\n",
       " 'Row id: 185: None',\n",
       " 'Row id: 186: None',\n",
       " 'Row id: 187: None',\n",
       " 'Row id: 188: None',\n",
       " 'Row id: 189: None',\n",
       " 'Row id: 190: None',\n",
       " 'Row id: 191: doctor: very good',\n",
       " 'Row id: 192: None',\n",
       " \"Row id: 193: ['doctor: careful']\",\n",
       " 'Row id: 194: None',\n",
       " \"Row id: 195: ['doctor: caring']\",\n",
       " \"Row id: 196: ['doctor: compassionate']\",\n",
       " \"Row id: 197: ['doctor: concerned']\",\n",
       " \"Row id: 198: ['doctor: considerate']\",\n",
       " \"Row id: 199: ['doctor: courteous']\",\n",
       " \"Row id: 200: ['doctor: courteous']\",\n",
       " \"Row id: 201: ['doctor: courteous']\",\n",
       " \"Row id: 202: ['doctor: courteous']\",\n",
       " \"Row id: 203: ['doctor: courteous']\",\n",
       " \"Row id: 204: ['doctor: courteous']\",\n",
       " 'Row id: 205: None',\n",
       " \"Row id: 206: ['doctor: courtesious']\",\n",
       " \"Row id: 207: ['doctor: courtesious']\",\n",
       " 'Row id: 208: None',\n",
       " \"Row id: 209: ['doctor: excellent']\",\n",
       " \"Row id: 210: ['doctor: excellent']\",\n",
       " \"Row id: 211: ['doctor: excellent']\",\n",
       " \"Row id: 212: ['doctor: excellent']\",\n",
       " \"Row id: 213: ['doctor: friendly']\",\n",
       " \"Row id: 214: ['doctor: kind']\",\n",
       " \"Row id: 215: ['doctor: kind']\",\n",
       " 'Row id: 216: None',\n",
       " 'Row id: 217: None',\n",
       " 'Row id: 218: None',\n",
       " 'Row id: 219: None',\n",
       " 'Row id: 220: None',\n",
       " 'Row id: 221: None',\n",
       " 'Row id: 222: None',\n",
       " 'Row id: 223: None',\n",
       " 'Row id: 224: None',\n",
       " 'Row id: 225: None',\n",
       " 'Row id: 226: None',\n",
       " 'Row id: 227: None',\n",
       " \"Row id: 228: ['doctor: short']\",\n",
       " 'Row id: 229: None',\n",
       " 'Row id: 230: None',\n",
       " \"Row id: 231: ['doctors: available']\",\n",
       " 'Row id: 232: None',\n",
       " 'Row id: 233: None',\n",
       " \"Row id: 234: ['doctor: amazing']\",\n",
       " 'Row id: 235: None',\n",
       " \"Row id: 236: ['doctor: awesome']\",\n",
       " \"Row id: 237: ['doctor: awesome']\",\n",
       " \"Row id: 238: ['doctor: caring']\",\n",
       " 'Row id: 239: None',\n",
       " \"Row id: 240: ['doctor: concerned']\",\n",
       " 'Row id: 241: None',\n",
       " 'Row id: 242: None',\n",
       " \"Row id: 243: ['doctor: efficient']\",\n",
       " \"Row id: 244: ['doctor: efficient']\",\n",
       " \"Row id: 245: ['doctor: efficient']\",\n",
       " \"Row id: 246: ['doctor: efficient']\",\n",
       " \"Row id: 247: ['doctor: excellent']\",\n",
       " \"Row id: 248: ['doctor: excellent']\",\n",
       " \"Row id: 249: ['doctor: excellent']\",\n",
       " 'Row id: 250: doctor was exceptionally diligent',\n",
       " \"Row id: 251: ['doctor: expeditious']\",\n",
       " 'Row id: 252: []',\n",
       " 'Row id: 253: doctor was extremely friendly',\n",
       " 'Row id: 254: doctor was extremely friendly',\n",
       " 'Row id: 255: doctor was extremely professional',\n",
       " 'Row id: 256: doctor was extremely thorough',\n",
       " \"Row id: 257: ['doctor: friendly']\",\n",
       " \"Row id: 258: ['doctor: friendly']\",\n",
       " \"Row id: 259: ['doctor: friendly']\",\n",
       " \"Row id: 260: ['doctor: gentle']\",\n",
       " \"Row id: 261: ['doctor: genuine']\",\n",
       " \"Row id: 262: ['doctor: good']\",\n",
       " \"Row id: 263: ['doctor: great']\",\n",
       " \"Row id: 264: ['doctor: great']\",\n",
       " \"Row id: 265: ['doctor: great']\",\n",
       " \"Row id: 266: ['doctor: great']\",\n",
       " \"Row id: 267: ['doctor: informative']\",\n",
       " 'Row id: 268: None',\n",
       " \"Row id: 269: ['doctor: empathetic']\",\n",
       " 'Row id: 270: None',\n",
       " 'Row id: 271: doctor was not friendly',\n",
       " 'Row id: 272: None',\n",
       " \"Row id: 273: ['doctor: polite', 'me: relaxed']\",\n",
       " \"Row id: 274: ['doctor: professional']\",\n",
       " \"Row id: 275: ['doctor: professional']\",\n",
       " \"Row id: 276: ['doctor: professional']\",\n",
       " \"Row id: 277: ['doctor: professional']\",\n",
       " \"Row id: 278: ['doctor: professional']\",\n",
       " \"Row id: 279: ['doctor: professional']\",\n",
       " \"Row id: 280: ['doctor: sincere']\",\n",
       " \"Row id: 281: ['doctor: prompt']\",\n",
       " \"Row id: 282: ['doctor: prompt']\",\n",
       " \"Row id: 283: ['doctor: quick']\",\n",
       " \"Row id: 284: ['doctor: quick']\",\n",
       " \"Row id: 285: ['doctor: quick', 'me: comfortable']\",\n",
       " \"Row id: 286: ['doctor: caring']\",\n",
       " 'Row id: 287: doctor was so considerate',\n",
       " 'Row id: 288: doctor was so friendly',\n",
       " 'Row id: 289: doctor was so great',\n",
       " 'Row id: 290: doctor was so nice',\n",
       " 'Row id: 291: doctor was so nice',\n",
       " \"Row id: 292: ['doctor: nice']\",\n",
       " 'Row id: 293: None',\n",
       " 'Row id: 294: None',\n",
       " \"Row id: 295: ['doctor: friendly']\",\n",
       " \"Row id: 296: ['doctor: terrific']\",\n",
       " 'Row id: 297: None',\n",
       " 'Row id: 298: None',\n",
       " 'Row id: 299: None',\n",
       " \"Row id: 300: ['doctor: uninterested']\",\n",
       " \"Row id: 301: ['doctor: considerate']\",\n",
       " \"Row id: 302: ['doctor: accomodating']\",\n",
       " 'Row id: 303: doctor was very attentive',\n",
       " 'Row id: 304: doctor was very calm',\n",
       " \"Row id: 305: ['doctor: caring']\",\n",
       " \"Row id: 306: ['doctor: caring']\",\n",
       " 'Row id: 307: doctor was very caring',\n",
       " 'Row id: 308: doctor was very caring',\n",
       " 'Row id: 309: doctor was very casual',\n",
       " 'Row id: 310: doctor was very clear',\n",
       " 'Row id: 311: doctor was very clear',\n",
       " 'Row id: 312: doctor was very comforting',\n",
       " 'Row id: 313: doctor was very congenial',\n",
       " 'Row id: 314: doctor was very considerate',\n",
       " 'Row id: 315: doctor was very convenient',\n",
       " 'Row id: 316: doctor was very cordial',\n",
       " 'Row id: 317: doctor was very courteous',\n",
       " 'Row id: 318: doctor was very courteous',\n",
       " 'Row id: 319: doctor was very courteous',\n",
       " 'Row id: 320: doctor was very curtious',\n",
       " 'Row id: 321: doctor was very dedicated',\n",
       " 'Row id: 322: doctor was very detailed',\n",
       " 'Row id: 323: doctor was very disrespectful',\n",
       " 'Row id: 324: doctor was very efficient',\n",
       " \"Row id: 325: ['doctor: fast']\",\n",
       " \"Row id: 326: ['doctor: fast']\",\n",
       " 'Row id: 327: doctor was very friendly',\n",
       " 'Row id: 328: doctor was very friendly',\n",
       " 'Row id: 329: doctor was very friendly',\n",
       " 'Row id: 330: doctor was very friendly',\n",
       " 'Row id: 331: doctor was very friendly',\n",
       " 'Row id: 332: doctor was very friendly',\n",
       " 'Row id: 333: doctor was very friendly',\n",
       " 'Row id: 334: doctor was very friendly',\n",
       " 'Row id: 335: doctor was very friendly',\n",
       " 'Row id: 336: doctor was very friendly',\n",
       " 'Row id: 337: doctor was very friendly',\n",
       " 'Row id: 338: doctor was very friendly',\n",
       " 'Row id: 339: doctor was very friendly',\n",
       " 'Row id: 340: doctor was very friendly',\n",
       " 'Row id: 341: doctor was very friendly',\n",
       " 'Row id: 342: doctor was very friendly',\n",
       " 'Row id: 343: doctor was very friendly',\n",
       " 'Row id: 344: doctor was very friendly',\n",
       " 'Row id: 345: doctor was very good',\n",
       " 'Row id: 346: doctor was very helpful',\n",
       " 'Row id: 347: doctor was very helpful',\n",
       " 'Row id: 348: doctor was very helpful',\n",
       " 'Row id: 349: doctor was very helpful',\n",
       " 'Row id: 350: doctor was very helpful',\n",
       " 'Row id: 351: doctor was very helpful',\n",
       " 'Row id: 352: doctor was very helpful',\n",
       " 'Row id: 353: doctor was very helpful',\n",
       " 'Row id: 354: doctor was very helpful',\n",
       " 'Row id: 355: doctor was very helpfull',\n",
       " 'Row id: 356: doctor was very informative',\n",
       " 'Row id: 357: doctor was very informative',\n",
       " \"Row id: 358: ['doctor: kind']\",\n",
       " \"Row id: 359: ['doctor: kind']\",\n",
       " 'Row id: 360: None',\n",
       " \"Row id: 361: ['doctor: kind']\",\n",
       " \"Row id: 362: ['doctor: kind']\",\n",
       " \"Row id: 363: ['doctor: kind']\",\n",
       " \"Row id: 364: ['doctor: kind']\",\n",
       " \"Row id: 365: ['doctor: kind']\",\n",
       " \"Row id: 366: ['doctor: kind']\",\n",
       " \"Row id: 367: ['doctor: kind']\",\n",
       " \"Row id: 368: ['doctor: kind']\",\n",
       " \"Row id: 369: ['doctor: kind']\",\n",
       " 'Row id: 370: doctor was very knowledgeable',\n",
       " 'Row id: 371: doctor was very knowledgeable',\n",
       " 'Row id: 372: doctor was very knowledgeable',\n",
       " 'Row id: 373: doctor was very knowledgeable',\n",
       " 'Row id: 374: doctor was very knowledgeable',\n",
       " 'Row id: 375: doctor was very nice',\n",
       " 'Row id: 376: doctor was very nice',\n",
       " 'Row id: 377: doctor was very nice',\n",
       " 'Row id: 378: doctor was very nice',\n",
       " 'Row id: 379: doctor was very nice',\n",
       " 'Row id: 380: doctor was very nice',\n",
       " 'Row id: 381: doctor was very nice',\n",
       " 'Row id: 382: doctor was very nice',\n",
       " 'Row id: 383: doctor was very quick',\n",
       " 'Row id: 384: doctor was very thorough',\n",
       " 'Row id: 385: doctor was very welcoming',\n",
       " 'Row id: 386: None',\n",
       " \"Row id: 387: ['doctor: wonderful']\",\n",
       " \"Row id: 388: ['Dr.: patient']\",\n",
       " 'Row id: 389: None',\n",
       " \"Row id: 390: ['experience: swift']\",\n",
       " \"Row id: 391: ['hospital: warm']\",\n",
       " 'Row id: 392: None',\n",
       " 'Row id: 393: nurse was very companionate',\n",
       " \"Row id: 394: ['MA: short']\",\n",
       " 'Row id: 395: None',\n",
       " \"Row id: 396: ['practitioner: sure', 'I: comfortable']\",\n",
       " 'Row id: 397: None',\n",
       " 'Row id: 398: None',\n",
       " 'Row id: 399: None',\n",
       " 'Row id: 400: None',\n",
       " 'Row id: 401: None',\n",
       " 'Row id: 402: nurse was very rude',\n",
       " \"Row id: 403: ['odor: noxious']\",\n",
       " 'Row id: 404: office: very clean',\n",
       " 'Row id: 405: office: very clean; customer service: great; customer service: speedy',\n",
       " 'Row id: 406: office: very clean',\n",
       " \"Row id: 407: ['tech: nice']\",\n",
       " 'Row id: 408: None',\n",
       " 'Row id: 409: None',\n",
       " \"Row id: 410: ['staff: attentive']\",\n",
       " 'Row id: 411: None',\n",
       " 'Row id: 412: None',\n",
       " \"Row id: 413: ['staff: courteous']\",\n",
       " 'Row id: 414: None',\n",
       " 'Row id: 415: None',\n",
       " \"Row id: 416: ['staff: professional']\",\n",
       " 'Row id: 417: None',\n",
       " 'Row id: 418: staff were very thorough',\n",
       " 'Row id: 419: None',\n",
       " \"Row id: 420: ['They: great']\",\n",
       " 'Row id: 421: None',\n",
       " 'Row id: 422: None',\n",
       " 'Row id: 423: None',\n",
       " 'Row id: 424: []',\n",
       " 'Row id: 425: None',\n",
       " 'Row id: 426: []',\n",
       " \"Row id: 427: ['doctor: kind']\",\n",
       " \"Row id: 428: ['we: fortunate', 'Neil: able']\",\n",
       " 'Row id: 429: None',\n",
       " 'Row id: 430: None',\n",
       " 'Row id: 431: None',\n",
       " 'Row id: 432: nurse was too busy',\n",
       " 'Row id: 433: None',\n",
       " 'Row id: 434: None',\n",
       " 'Row id: 435: None',\n",
       " 'Row id: 436: []',\n",
       " 'Row id: 437: None']"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
